# Core Knowledge

## K1 - The subject material

Computing is a highly technical field and teaching it requires a great deal of knowledge. My background working in software development allows me to bring real world experience in to the classroom. This influences how I teach programming and software engineering related subjects. The difference between software engineering theory and software development in practice can be significant and being able to bring that awareness to the students is beneficial.  

During the revalidation of our programmes we were able to update our curriculum based on developments in technology, software and programming.  

**Relavant Pages : ** [Modules](../modules.md), [Game Development Revalidation](../CaseStudies/GamesDevRevalidation.md), [Cross Discipline Collaboration](../CaseStudies/CrossDisciplineCollaboration.md)  

## K2 - Appropriate methods for teaching, learning and assessing in the subject area and at the level of the academic programme

Computing as an industry, especially software development, requires individual knowledge and ability, alongside teamwork which utilises differening knowledge and ability in team members. This poses a number of challenges teaching computing, especially in teaching programming. Is it possible to develop individual ability whilst also teaching efficient and smart use of differing knowledge in teams.  

Our approach is based around problem solving. Problem based learning is student focused and allows them to learn by doing. It is a form of Active learning - *"a method of learning in which students are actively or experientially involved in the learning process and where there are different levels of active learning, depending on student involvement."* (Bonwell & Eison 1991).

**Relavant Pages : ** [Pedagogy](../CaseStudies/pedagogy.md), [Online Teaching](../CaseStudies/OnlineTeaching.md), [Marking](../CaseStudies/Marking.md), [Learning Requirements](../CaseStudies/learningrequirements.md)

## K3 - How students learn, both generally and within their subject/disciplinary area(s)

In Computing at YSJ we have a wide range of students. Computing Science is not widely taught at school level so we have to assume no knowledge of computing and programming at the start of first year. However some students do study computer science and programming at school and others have taught themselves to program. This leads to a wide range of ability levels in first year students (who have come directly from school). Similarly mature students, and others who have not come directly from school, have a wide range of experience and knowledge. The challenge here is in getting all of the students to the same base level by the end of first year, getting those with no experience programming up to speed while maintaining the engagement and providing some challenge to those with significant experience.  

We have found that problem based learning is the best approach. We can provide a range of problems to the students that cover the course material and additional problems that give the experienced students the opportunity to push themselves further.  

Having the students work together in twos or threes works well with the problem based approach. Working together allows students with differing knowledge and understanding to work together to find solutions. Additionally many of the more knowledgable students enjoy helping others learn.

We have resources in place to help students with learning challenges. Our timetable is designed with some flexibility to help carers schedule their studies. We use Learning Support Plans (LSPs) to help students with specific requirements.

**Relavant Pages : ** [Pedagogy](../CaseStudies/pedagogy.md), [Learning Requirements](../CaseStudies/learningrequirements.md), [Online Teaching](../CaseStudies/OnlineTeaching.md)  

## K4 - The use and value of appropriate learning technologies

YSJ uses Moodle as its electronic blackboard system. We use Moodle to host lecture slides, class exercises, assessments and other content.  

**Relavant Pages : **  [Online Teaching](../CaseStudies/OnlineTeaching.md)

## K5 - Methods for evaluating the effectiveness of teaching 

The first method for evaluating the effectiveness of teaching is the marks students achieve in the assessments. Generally good passing grades indicate effective teaching but they could also be an indication that the module and/or the assessment was pitched at too low a level, that is it was too easy.  

We elicit feedback directly from students in a number of ways ([Student experience surveys](https://www.yorksj.ac.uk/students/policies-and-documents/student-experience-surveys-and-module-evaluation/)).  

* Formative assessments. I've tried including formative assessments in modules but I find if they aren't compulsory (i.e. they don't count towards the final grade) then very students submit anything for them (two or three out of fifty). Our modules are generally assessed as portfolios and by University regulations we cannot require any part of the portfolio to be submitted before the whole portfolio is submitted. Studies suggest automated formative assessment and feedback are effective (Barana et al 2016, 2019, Pankiewicz and Furmańczyk 2020, Spector et al 2016), engaging students and aiding progress. I do this through [unit testing](https://www.techtarget.com/searchsoftwarequality/definition/unit-testing). Unit testing is a software development technique where automated tests can be written for code before the code is even written. Where possible the exercises and assessments I create use include unit tests. This gives students a framework to work within and instant feedback on their solutions. Unit testing is not always applicable though. Setting up unit tests requires me to build a framework for the students to complete. For some exercises this is providing too much information - sometimes students need to build the framework themselves.  
  
  An informal form of formative feedback is used everyweek. The typical structure of a programming class is a lecture followed by a lab session. During the lab session students work on exercises based on the lecture. I answer any questions they have as and when they come up. This means I usually spend time sat with each student, at thier computer, looking at their code and helping them. This may take the form of helping them work through the exercise problem, helping them understand why their solution doesn't work, or confirming they are on the right track when they are not sure. 
* Module leads carry out mid-module evaluations for their modules. This takes place mid-way through the semester in the form of an anonymous questionnaire. For first year students we ask three questions; what should we keep doing? what should we change? what should we start doing? For second and third years the questionnaire is more involved and asks a series of specific questions. We are discussing changing the format of the questionnaires to match up with the end of year evaluations (YES) and the NSS questionnaire. More similarity wouild help us target changes to better improve our NSS scores but the longer and more detailed the questionnaire is the less likely students are to fill it in. The mid module evaluation helps us judge how the module is progressing each year. We can use the feedback to make changes to the module immeadiately rather than in the next academic year.  
* End of year feedback. We used to run end of module evaluations on a module by module basis. The university has now changed to an end of year survey - the YSJ Experience Survey (YES). The YES survey is modelled after the NSS survey and asks about the year as a whole rather than focusing on specific modules. It asks about every aspect of student life, not just the taught subjects. This has value in the general sense but we have lost the specific module feedback we used to have. We have considered implementing our own end of module surveys, I suspect that students would not take kindly to even more surveys to be filled in.  
* The National Student Survey (NSS) is sent to all students in their final year. It provides a consistant set of feedback to every department of every university. It informs University policy and spending and is used to direct departments focus to specific issues. Our results (in Computing) are generally pretty mediocre. Our relatively low student numbers mean we are susceptible to large swings in our results. 
* We take part in the University's Peer Observation of Learning and Teaching ([POLT](https://www.yorksj.ac.uk/staff/learning-and-teaching/peer-observation-of-learning-and-teaching/)) programme. The POLT is *"viewed as a collaborative non-judgemental process involving two or more peers who mutually benefit from the dialogue that takes place."* It takes different forms from discussions between peers to peer observation of teaching to feedback. The process pairs academics with colleagues from different departments with the intention to spread best practices. I've found the discussions to be valuable, especially the discussions of formal pedagogy.  
* All our second and third year modules are assessed by an external examiner. Our external examiners are colleagues from other universities and have experience and knowledge of the computing field. They provide us with invaluable feedback on the quality of our modules, our assessments and our marking.  
* Reflection. Reflection is essential. Rolfe (Rolfe et al 2001) describe a cycle of what? so what? and now what? for reflection. Greatly simplified this is "what am I reflecting on?", "so what happened?" and "now what am I going to do to improve/change this?". I take this approach, informally, when reflecting on my teaching and its effectiveness.
  During the semester the module is taught I try to keep track of how the students are progressing. When planning the sessions for a module I usually choose a topic that is useful but not essential to the module. Something that pushes a little beyond the learning outcomes. If I find that the students aren't progressing as well as expected I know I can drop that lecture and insert a revision lecture earlier on. Very occasionally I find the students progressing better than expected and I can push two lectures together to challenge them, extending the extra material at the end. I keep notes on any topics that proved tricky to understand, any new examples, etc.
  At the end of the semester, once the marking is completed, I think about the module again, thinking in terms of the assessment results. Do the results suggest a problem in any particular area? Do they suggest an area that is too easy? Is there a concept that the students are misunderstanding or avoiding? These feed in to any changes/updates for the next year.  

**Relavant Pages : ** [Marking](../CaseStudies/Marking.md), [Pedagogy](../CaseStudies/pedagogy.md)  

### Evidence

* [Games Fundamentals 2019-20 Mid Module Evaluation](../evidence/GameFun201920MidMod.png)
* [Games Fundamentals 2020-21 Mid Module Evaluation](../evidence/GameFun202021MidMod.pdf)
* [Games Fundamentals 2021-22 Mid Module Evaluation](../evidence/GameFun202122MidMod.pdf)
* [Programming 02 2020-21 Mid Module Evaluation](../evidence/Prog02-2021MidMod.pdf)
* [Programming 02 2021-22 Mid Module Evaluation](../evidence/Prog02-2122MidMod.pdf)
* [Programming 03 2020-21 Mid Module Evaluation](../evidence/Prog03MME2021.pdf)
* [Programming 03 2021-22 Mid Module Evaluation](../evidence/Prog03MME2122.pdf)
* [Assessment Brief Example](../evidence/AssessmentBriefExample.docx)
* [Marking Spreadsheet Example (anonymised)](../evidence/MarksExample.xlsm)
* [Feedback Example (anonymised)](../evidence/FeedbackExample.pdf)

## K6 - The implications of quality assurance and quality enhancement for academic and professional practice with a particular focus on teaching

* External examiner feedback. Our second and third year modules are all externally examined by colleagues from other universities. Their feedback is invaluable. I have received good feedback on the feedback I give students. Our current examiner has questioned the use of external websites which has, in part, led to me returning to just using Moodle.  
* National Student Survey (NSS). The NSS was discussed in the section above (K5). The scores and gradings in the NSS are used to dictate University policy which in turn directs focus to specific issues. The NSS is a valuable tool for evaluating the sector as a whole and universities find it essential for overseas student recruitment, though it seems UK students are largely unaware of the NSS until they are asked to complete the survey in their final year.
* Games Development Revalidation
* BCS acreditation

**Relavant Pages : ** [Game Development Revalidation](../CaseStudies/GamesDevRevalidation.md), [BCS Accreditation](../CaseStudies/BCSAccreditation.md)

### Evidence

* [Consultation With Careers & Student Services](../evidence/ConsultationwiththeCareersandStudentOpportunitiesteam.pdf)

## References

* A. Barana, M. Marchisio and S. Rabellino, "Empowering Engagement through Automatic Formative Assessment," 2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC), 2019, pp. 216-225, doi: 10.1109/COMPSAC.2019.00040. 
* Barana, A. and Marchisio, M., 2016. Ten good reasons to adopt an automated formative assessment model for learning and teaching Mathematics and scientific disciplines. Procedia-Social and Behavioral Sciences, 228, pp.608-613.
* Barrows, Howard S. (1996). "Problem-based learning in medicine and beyond: A brief overview". New Directions for Teaching and Learning. 1996 (68): 3–12. doi:10.1002/tl.37219966804.
* Bonwell, C. C., & Eison, J. A. (1991). Active Learning: Creating Excitement in the Classroom. ASHE-ERIC Higher Education Report, Washington DC: School of Education and Human Development, George Washington University.
* Pankiewicz, M. and Furmańczyk, K., 2020, June. From Zero to Hero–Automated Formative Assessment for Supporting Student Engagement and Performance in a Gamified Online Programming Course. In EdMedia+ Innovate Learning (pp. 1252-1261). Association for the Advancement of Computing in Education (AACE).
* Rolfe, G., Freshwater, D., Jasper, M. (2001) Critical reflection in nursing and the helping professions: a user’s guide. Basingstoke: Palgrave Macmillan.
* Spector, J.M., Ifenthaler, D., Sampson, D., Yang, J.L., Mukama, E., Warusavitarana, A., Dona, K.L., Eichhorn, K., Fluck, A., Huang, R. and Bridges, S., 2016. Technology enhanced formative assessment for 21st century learning.
* Wood, D. F. (2003). "ABC of learning and teaching in medicine: Problem based learning". BMJ. 326 (7384): 328–330. doi:10.1136/bmj.326.7384.328. PMC 1125189. PMID 12574050.
* "Problem-Based Learning - Education - Maastricht University". www.maastrichtuniversity.nl. Retrieved 2022-08-15.
